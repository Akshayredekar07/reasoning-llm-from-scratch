
think like you are the writing book and you have to gvie this in proepr latex format so give in proper reablde content book format gvie this notes in latex code so i can complile this and download pdf
can you gvie this content in latex format
give this content in proper formatting only dont skip the content gvie also diagrma in text formatÂ 

Certainly! Here is the text extracted from the image:

---

## Extracted Text

Lecture 4: Introduction to **Reinforcement Learning**

So far, we have covered the following methods for inducing reasoning in LLMs:

(1) Inference-Time Compute Scaling $\checkmark$

Today, we will start to understand the second method for inducing reasoning in LLMs â€“ **Pure Reinforcement Learning**

We will begin our journey by understanding first about classical reinforcement learning:

The **Reinforcement Learning Problem**:

**Reinforcement Learning: An Introduction**

Second edition, in progress

Richard S. Sutton and Andrew G. Barto
$\copyright$ 2014, 2015

To understand about the RL problem, it would help us to understand how different is it from supervised learning and unsupervised learning.

---

The image appears to be from lecture notes or a presentation about Reinforcement Learning, citing the famous book by Sutton and Barto.Here is the text extracted from the image:

---

## Extracted Text

**Supervised Learning:**

[Diagram showing:] **Labelled data** (Malignant/Benign tumor images) $\to$ **AI Model** $\to$ **New Image** (unseen data) **Prediction on Unseen data** (Benign or Malignant?)

Objective: Be able to **Generalize** or **extrapolate** in situations not present in the training dataset.

---

**Unsupervised Learning:**

[Diagram showing:] **Iris flowers (No labels)** $\to$ **AI MODEL** $\to$ **Clustering of data** (Iris Dataset Four-Dimensional Visualization)

Objective: **Finding structures hidden in collections of unlabelled data**

Now, let us understand how reinforcement learning compares to both these categories

**Labelling of data:**

---Here is the text extracted from the image:

---

## Extracted Text

| Supervised | Unsupervised | Reinforcement |
|:---:|:---:|:---:|
| Labelled | Unlabelled | Model learns from interaction. It is impractical to obtain examples of correct behavior in all situations encountered. |

**Objective:**-

| Supervised | Unsupervised | Reinforcement |
|:---:|:---:|:---:|
| Generalize to situations not present in the training data. | find hidden structures in collections of unlabelled data. | Maximize a reward signal. |

Reinforcement learning problems involve learning what to do - how to map **situations** to **actions**, so as to maximize a **reward** signal.

Of all the forms of machine learning, reinforcement learning is the closest to the kind of learning which humans and other animals do. Many of its core algorithms were inspired from biological learning systems.

1960-1980: Methods based on "search" or "learning" were classified as "**weak methods**."

Reinforcement Learning signified a change in this way of thinking. Let us look at some real-life examples of Reinforcement learning to understand this better.

---Here is the text extracted from the image:

---

## Extracted Text

The following examples have guided the development of **Reinforcement Learning** as a field:

**(1) A master chess player makes a move:-**

Â The choice of the move is based on 2 things:
(1) **Planning** by anticipating replies and counterreplies.
(2) **Intuitive judgements** about the desirability of moves.

**(2) How does a gazelle (animal) calf learn to run:**

Â After birth:
$\to$ 2-3 minutes later: Struggles to get up.
$\to$ 30 minutes later: Runs at 36 km/hr.

**(3) How does a mobile robot make decisions**

| **Decision Questions** | **Decision Parameters** |
|:---:|:---:|
| Should I go to a new room in search of trash or go back to charging station? | Current battery charge |
| | How much time it has taken to find recharger in the past? |

**(4) Phil prepares his breakfast**

---Here is the text extracted from the image:

---

## Extracted Text


**Actions**: Walking to the cupboard, opening it, selecting a cereal box, reaching for, grasping and retrieving the box, obtain a plate, spoon.

Each step involves a series of eye-movements to obtain information and to guide reaching and locomotion. Each step is guided by **goals** in service of other goals with the final goal of obtaining nourishment.

**What is common in all these examples?**

[Diagram showing:] **Active decision making agent** $\xrightarrow{\text{Interaction}}$ **Environment** $\to$ **Goal**

| Active decision making agent | Environment | Goal |
|:---:|:---:|:---:|
| Chess player, gazelle, mobile robot, Phil | Chess Board, Nature + Internal for gazelle, Room + Battery, Kitchen + Internal* for Phil | Win game, run fast, max. trash collection, obtain nourishment |

\* refers to the agent's internal memories, preferences which also form a part of the environment.

In all these examples, the **agent** uses its **experience** to improve its performance over time by **interacting with the environment**. Think how?ğŸ’¡

---Here is the text extracted from the image:

---

## Extracted Text

**Elements of Reinforcement Learning:**-

**(1) Policy:** Informally, **policy** defines the agent's way of behaving at a given time.

**(2) Reward Signal:** **Reward signal** defines the goal in a reinforcement learning problem. At each time step, the environment sends to the reinforcement learning agent, a single number - a **reward**. The sole objective of the agent is to maximize the total rewards received over time.

Â Reward is analogous to pleasure or pain in a biological system. If you touch a boiling vessel, your body gives you a negative **reward signal**.

**(3) Value function:**- While reward signals indicate what is good in an immediate sense, **Value function** specifies what is good in the **long run**.

**Value** of a state is the total amount of reward the agent can expect to accumulate over the future, starting from that state.

As an example, consider a sports tournament in which there are 14 matches. Let us say, a player $\text{X}$ is selected by a team. Now, in the first 2 matches, this player does not play well. The **reward signal** is low. But the captain has faith in the player and believes that he will contribute in later matches. Hence, even if the **reward** signal is low, the **value** is high, since the long-term desirability of the player is high.

---Here is the text extracted from the image:

---

## Extracted Text

Â $\to$ **Reward signal vs Value Function**

In the field of reinforcement learning, the central role of **value estimation** is the most important thing which researchers learnt from 1960-1990.

**(4) Model of the environment:-**

**Model** is something which mimics the behavior of the environment.

[Diagram showing:] **State, Action** $\to$ **Model** $\to$ **Next State, Next Reward**

There are **model-based** (used for planning) as well as **model-free** methods in reinforcement learning:

A Practical Example:- **Game of Tic-Tac-Toe**

We will understand the general idea of reinforcement learning using this example:

[Diagram of a Tic-Tac-Toe board:]
$\text{X}|\text{O}|\text{O}$
$\text{X}|\quad|\text{O}$
$\text{X}|\quad|\text{X}$

$\text{X}$: Player 1 (Us)
$\text{O}$: Player 2 (Opponent)

---Here is the text extracted from the image:

---

## Extracted Text

Â $\to$ **Reward signal vs Value Function**

In the field of reinforcement learning, the central role of **value estimation** is the most important thing which researchers learnt from 1960-1990.

**(4) Model of the environment:-**

**Model** is something which mimics the behavior of the environment.

[Diagram showing:] **State, Action** $\to$ **Model** $\to$ **Next State, Next Reward**

There are **model-based** (used for planning) as well as **model-free** methods in reinforcement learning:

A Practical Example:- **Game of Tic-Tac-Toe**

We will understand the general idea of reinforcement learning using this example:

[Diagram of a Tic-Tac-Toe board:]
$\text{X}|\text{O}|\text{O}$
$\text{X}|\quad|\text{O}$
$\text{X}|\quad|\text{X}$

$\text{X}$: Player 1 (Us)
$\text{O}$: Player 2 (Opponent)

---Here is the text extracted from the image:

---

## Extracted Text

Now, let us see how we modify the **value function estimates** as we play the game more number of times such that they reflect the true probabilities.


Let us say, we are here. Now, to play our next move, we look at all possible next states and select the one with highest probability.

**All Possible Next States:**-

[Series of partial Tic-Tac-Toe boards, showing the next move 'X' in different positions]

| State 1 | State 2 | State 3 | State 4 |
|:---:|:---:|:---:|:---:|
| Value function: $\text{0.5}$ | $\text{0.6}$ | $\text{0.55}$ | $\text{0.4}$ |

This is called **exploitation**: Selecting the state with the greatest value.

While we are playing, we change the value of the states we find ourselves. We make them more **accurate estimates** of the probability of winning.

ğŸ’¡ **IMPORTANT INTUITION:-**

After each **greedy move**, the current value of the earlier state is adjusted to be closer to the value of the latter state.

This is done by moving the earlier state value a fraction of a way towards the latter state. This is called **"backing-up"**.

---Here is the text extracted from the image:

---

## Extracted Text

Now, let us see how we modify the **value function estimates** as we play the game more number of times such that they reflect the true probabilities.


Let us say, we are here. Now, to play our next move, we look at all possible next states and select the one with highest probability.

**All Possible Next States:**-

[Series of partial Tic-Tac-Toe boards, showing the next move 'X' in different positions]

| State 1 | State 2 | State 3 | State 4 |
|:---:|:---:|:---:|:---:|
| Value function: $\text{0.5}$ | $\text{0.6}$ | $\text{0.55}$ | $\text{0.4}$ |

This is called **exploitation**: Selecting the state with the greatest value.

While we are playing, we change the value of the states we find ourselves. We make them more **accurate estimates** of the probability of winning.

ğŸ’¡ **IMPORTANT INTUITION:-**

After each **greedy move**, the current value of the earlier state is adjusted to be closer to the value of the latter state.

This is done by moving the earlier state value a fraction of a way towards the latter state. This is called **"backing-up"**.

---Here is the text and diagrams extracted from the image:

---

## Extracted Text and Diagrams

[**Diagram 1: Game Tree**]

The diagram shows a game tree for Tic-Tac-Toe moves starting from an initial position. The path followed by the agent (our move) is highlighted.

* $\to$ **sequence of Tic-Tac-Toe moves.**
Â  Â  * **Exploitation** (Move from $a \to b$ where $b$ is the state with the highest value, then from $c \to d$)
Â  Â  * **Exploration** (Move from $d \to e'$ where $e'$ is a non-greedy, or randomly chosen move)
Â  Â  * **Exploitation** (Move from $f \to g$, where $g$ is the highest value state)

*The notation $c \to d$ and $g \to \text{dropped}$ suggests the path taken by the agent through the game tree.*

[**Diagram 2: Game Tree (Alternative/Second Play)**]

The diagram shows a second sequence of moves in a game tree, possibly illustrating a different game or a revised strategy.

* $\to$ **sequence of Tic-Tac-Toe moves.**
Â  Â  * **Exploitation**
Â  Â  * **Exploration**

---
**Some Mathematics:-**

Let $\text{s}$ denote the state before the greedy move and $\text{s}'$ after the greedy move. Let $\text{V}(s)$ denote the value function:
$\text{V}(s) \to$
$\text{V}(s')$

The update rule to match the "intuition" above is given by,

---Here is the text and mathematical notation extracted from the image:

---

## Extracted Text and Formula

Let $\mathbf{s}$ denote the state before the greedy move and $\mathbf{s}'$ after the greedy move. Let $\mathbf{V}(s)$ denote the value function.

The update rule to match the "intuition" above is given by,

$$\mathbf{V}(s) \leftarrow \mathbf{V}(s) + \alpha \mathbf{[V(s') - V(s)]}$$

where $\alpha \rightarrow$ small positive fraction

*Example Calculation:*
$$\mathbf{V}(s) = 0.8 + \alpha [0.9 - 0.8] = 0.8 + \alpha [0.1]$$*If $\alpha = 0.01$:*$$0.8 + 0.001 = 0.801$$

Using this update rule, the value function converges to the true probabilities of winning from each state.

**What did we learn from this example?**

(1) In **reinforcement learning**, the learning happens by **interacting with the environment**, the opponent player.

(2) There is a clear **goal** and correct behavior includes **planning**, especially delayed effects of one's choices.

(3) This was an example of a "**model-free** reinforcement learning. We did not use any model of the opponent.

**A Brief History of Reinforcement Learning:**-

---# A Brief History of Reinforcement Learning

## Timeline/Flowchart of Reinforcement Learning History

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â”‚Â  Thorndike - Trial and Error Learning (1911)Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â”‚Â  Pavlov - 1927 - Reinforcement CoiningÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â”‚Â  Alan Turing (1948) - Pleasure Pain SystemÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â”‚Â  OPTIMAL CONTROL (1950s)Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Â  Â  Â  Â  Â  Â  Â  â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  â†“Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”Â  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚Â  â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â”‚Â  Dynamic Programming,Â  Â  â”‚Â  â”‚Â  Markov Decision ProcessesÂ  Â  Â  â”‚
â”‚Â  Bellman (1950s)Â  Â  Â  Â  Â â”‚Â  â”‚Â  (1950s)Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚
â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚Â  â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜Â  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Andreae (1963) - StELLAÂ  Â  Â  Â  Â  Â  â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Klopf (1972) - Link Trial andÂ  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Error to RLÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Paul Werbos (1977) - ConnectionÂ  Â  â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  between OC and DPÂ  Â  Â  Â  Â  Â  Â  Â  Â  â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Sutton, Barto (1980-1990) -Â  Â  Â  Â  â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Temporal-difference LearningÂ  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Watkins - RL + MDP (1989)Â  Â  Â  Â  Â  â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Watkins (1989) - Q learningÂ  Â  Â  Â  â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â†“
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Tesauro's (1992) - TD GammonÂ  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â”‚
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Note:** Several researchers have contributed to the field of reinforcement learning. Keep this flow in mind as we move ahead in the course.

---

### Key Milestones Summary:

- **1911** - Thorndike's Trial and Error Learning
- **1927** - Pavlov coins "Reinforcement"
- **1948** - Alan Turing's Pleasure Pain System concept
- **1950s** - Optimal Control, Dynamic Programming (Bellman), Markov Decision Processes
- **1963** - Andreae develops StELLA
- **1972** - Klopf links Trial and Error to RL
- **1977** - Paul Werbos connects Optimal Control and Dynamic Programming
- **1980-1990** - Sutton & Barto develop Temporal-difference Learning
- **1989** - Watkins combines RL with MDP and introduces Q-learning
- **1992** - Tesauro's TD-Gammon demonstrates practical success

---